"""
This module provides a generic, centralized service for handling streaming
requests to the Large Language Model (LLM).

It encapsulates the boilerplate logic for threading, progress bar management,
API call iteration, and error handling, allowing feature-specific modules
to be cleaner and more focused on their core logic.
"""
import threading
from tkinter import messagebox
from llm import state as llm_state
from llm import api_client as llm_api_client
from llm import utils as llm_utils
from utils import debug_console

def start_streaming_request(editor, prompt, model_name, on_chunk, on_success, on_error):
    """
    Starts a generic, non-blocking LLM streaming request in a background thread.
    Communicates via UI callbacks.
    """
    if llm_state._is_generating:
        messagebox.showinfo("LLM Busy", "LLM is currently generating. Please wait.")
        return

    # Set generating state immediately to prevent race conditions
    llm_state._is_generating = True
    llm_state._is_generation_cancelled = False

    progress_bar = llm_state._llm_progress_bar_widget
    
    def stream_thread_target():
        """Target function for the background thread."""
        accumulated_text = ""
        try:
            for response in llm_api_client.request_llm_generation(prompt, model_name=model_name):
                if llm_state._is_generation_cancelled:
                    break
                
                if response.get("success"):
                    chunk = response.get("chunk")
                    if chunk:
                        accumulated_text += chunk
                        editor.after(0, on_chunk, chunk)
                    
                    if response.get("done") and not llm_state._is_generation_cancelled:
                        final_text = accumulated_text
                        if "deepseek" in model_name:
                            final_text = llm_utils.strip_think_tags(final_text)
                        editor.after(0, on_success, final_text)
                        return
                else:
                    error_msg = response.get("error", "Unknown error during streaming.")
                    if not llm_state._is_generation_cancelled:
                        editor.after(0, on_error, error_msg)
                    return
        except Exception as e:
            error_msg = f"An unexpected error occurred in the streaming thread: {e}"
            debug_console.log(error_msg, level='ERROR')
            if not llm_state._is_generation_cancelled:
                editor.after(0, on_error, error_msg)
        finally:
            llm_state._is_generating = False
            if progress_bar and progress_bar.winfo_exists():
                editor.after(0, progress_bar.stop)
                editor.after(0, progress_bar.pack_forget)

    if progress_bar:
        progress_bar.pack(pady=2)
        progress_bar.start(10)
    
    threading.Thread(target=stream_thread_target, daemon=True).start()

def get_blocking_response(prompt, model_name):
    """
    Requests a single, blocking completion from the LLM.
    
    This is useful for internal operations that need the full response at once.
    It handles state, progress bar, and errors automatically.
    
    Returns:
        str: The complete text generated by the LLM, or an empty string on error.
    """
    if llm_state._is_generating:
        messagebox.showwarning("LLM Busy", "The LLM is currently busy. Please wait.")
        return ""

    # Set generating state immediately
    llm_state._is_generating = True
    llm_state._is_generation_cancelled = False
    
    progress_bar = llm_state._llm_progress_bar_widget
    if progress_bar:
        progress_bar.pack(pady=2)
        progress_bar.start(10)

    full_response = ""
    try:
        for response in llm_api_client.request_llm_generation(prompt, model_name=model_name):
            if llm_state._is_generation_cancelled:
                break
            
            if response.get("success"):
                chunk = response.get("chunk")
                if chunk:
                    full_response += chunk
                if response.get("done"):
                    break
            else:
                error_message = response.get("error", "Unknown error")
                messagebox.showerror("LLM Error", f"An error occurred: {error_message}")
                return ""
    except Exception as e:
        error_message = f"An unexpected error occurred during LLM request: {e}"
        debug_console.log(error_message, level='ERROR')
        messagebox.showerror("LLM Error", error_message)
        return ""
    finally:
        # Ensure state is reset and progress bar is hidden
        llm_state._is_generating = False
        if progress_bar and progress_bar.winfo_exists():
            progress_bar.stop()
            progress_bar.pack_forget()

    if "deepseek" in model_name:
        full_response = llm_utils.strip_think_tags(full_response)

    return full_response
