
"""
This module provides utility functions for interacting with the editor's content
and processing text generated by the Large Language Model (LLM).
It includes functions for extracting context and handling text overlaps.
"""
import tkinter as tk
from utils import logs_console

def extract_editor_context(editor_widget, lines_before_cursor=5, lines_after_cursor=5):
    """
    Extracts a specified number of lines of text context from the editor widget
    around the current cursor position.

    This function is useful for providing relevant surrounding text to the LLM
    when generating or completing text, without sending the entire document.

    Args:
        editor_widget (tk.Text): The Tkinter Text widget from which to extract context.
        lines_before_cursor (int, optional): The number of lines to extract before the cursor.
                                             Defaults to 5.
        lines_after_cursor (int, optional): The number of lines to extract after the cursor.
                                            Defaults to 5.

    Returns:
        str: A string containing the extracted text context, with lines separated by newlines.
             Returns an empty string if the editor widget is invalid or an error occurs.
    """
    if not editor_widget:
        logs_console.log("Editor widget is None. Cannot extract context.", level='WARNING')
        return ""

    try:
        # Get the current cursor position (e.g., "line.char").
        cursor_index = editor_widget.index(tk.INSERT)
        # Extract the current line number from the cursor index.
        current_line_number = int(cursor_index.split(".")[0])

        # Get the index of the last character in the document to determine total lines.
        last_line_index_str = editor_widget.index("end-1c")
        # Calculate total lines, handling empty documents where "end-1c" might be "1.0".
        total_lines_in_document = int(last_line_index_str.split(".")[0]) if last_line_index_str != "1.0" or editor_widget.get("1.0", "1.end").strip() else 0

        # Determine the starting and ending line numbers for context extraction.
        # Ensure they are within valid document bounds (1 to total_lines_in_document).
        start_line_for_context = max(1, current_line_number - lines_before_cursor)
        end_line_for_context = min(total_lines_in_document, current_line_number + lines_after_cursor)

        # Extract each line within the determined range and join them with newlines.
        context_lines = [editor_widget.get(f"{i}.0", f"{i}.end") for i in range(start_line_for_context, end_line_for_context + 1)]
        return "\n".join(context_lines)
    except Exception as e:
        logs_console.log(f"An error occurred while extracting editor context: {e}", level='ERROR')
        return ""

def remove_prefix_overlap_from_completion(text_before_completion, llm_generated_completion):
    """
    Removes redundant overlapping text from the beginning of an LLM-generated completion.

    Sometimes, an LLM might start its completion by repeating a portion of the input
    text it was given. This function identifies and removes such overlaps to ensure
    a clean and natural continuation of the text.

    Args:
        text_before_completion (str): The text that was provided to the LLM as context
                                      immediately preceding the desired completion point.
        llm_generated_completion (str): The raw text generated by the LLM.

    Returns:
        str: The LLM-generated completion with any overlapping prefix removed.
    """
    # Split both texts into words for easier comparison.
    words_in_context = text_before_completion.split()
    words_in_completion = llm_generated_completion.split()

    overlap_length = 0
    # Iterate to find the longest sequence of words that is a suffix of the context
    # Check longest matching sequence at prefix/suffix boundary
    # Loop from 1 up to minimum length of word lists
    for i in range(1, min(len(words_in_context), len(words_in_completion)) + 1):
        # Check if the last `i` words of the context match the first `i` words of the completion.
        if words_in_context[-i:] == words_in_completion[:i]:
            overlap_length = i # Update overlap length if a match is found.
            
    # Join the words of the completion starting from the end of the overlap.
    # .strip() is used to remove any leading/trailing whitespace that might result from joining.
    return " ".join(words_in_completion[overlap_length:]).strip()

def normalize_text_content(text: str) -> str:
    """
    Normalizes LLM-generated text by applying consistent cleaning rules.
    Used for both streaming chunks and final responses to ensure consistency.
    
    Args:
        text (str): Raw text from LLM
        
    Returns:
        str: Normalized text ready for display/storage
    """
    if not text:
        return ""
    
    import re
    
    # Remove markdown code blocks (common in LLM responses)
    text = re.sub(r'```(?:latex|tex|LaTeX)?\s*', '', text)
    text = text.replace('```', '')
    
    # Remove think tags (some models use these)
    text = re.sub(r'</?think>', '', text)
    
    # Remove surrounding quotes if they wrap the entire content
    text = text.strip()
    if len(text) >= 2:
        if (text.startswith('"') and text.endswith('"')) or (text.startswith("'") and text.endswith("'")):
            text = text[1:-1]
    
    # Normalize whitespace but preserve intentional LaTeX spacing
    text = text.strip()
    
    return text

def prepare_final_response(accumulated_chunks: str, final_raw_text: str) -> str:
    """
    Prepares the final response text, choosing the most appropriate source.
    Ensures consistency between streaming display and final accepted text.
    
    Args:
        accumulated_chunks (str): Text built from streaming chunks
        final_raw_text (str): Complete final text from LLM service
        
    Returns:
        str: Final normalized text for acceptance
    """
    # Use final_raw_text when available (more complete/accurate)
    # Fall back to accumulated_chunks for streaming-only scenarios
    source_text = final_raw_text if final_raw_text else accumulated_chunks
    return normalize_text_content(source_text)
